# county_subsidy_retrieval
This repository contains all the files related to retrieving US subsidy data from Subsidy Tracker, a watchdog organization that collects and organizes data on subsidies. I started this project because I wanted to study whether and how local US government use subsidies to mitigate the economic threat posed by globalization. The literature on this topic in political science is surprisingly bare, especially given how important globalization, immigration, industrial decline, and the working class have recently been in American politics. I thought it would be useful to study whether local governments distribute subsidies to dying firms in import-competing industries (e.g., manufacturing), or to thriving firms in export-competing industries (e.g., tech, science, advanced manufacturing). This is not only an interesting question as a matter of political economy (e.g., who produces what and who gets what), but also as a matter of helping governments figure out a better strategy for distributing the taxpayer's resources.< br/>

This repository only exists only Subsidy Tracker's data is hard to get. They graciously make their data publicly available but, understandably, they also limit how much data you can download at a time for the sake of their servers. Your choices for the data are to manually download 100 observations lots of times, purchase access and get their entire dataset, or write a web scraper to make infrequent pulls of their data while making sure to not flood their servers. I went for the latter option. The files in this repository contain the .R files I wrote to do the web scraping. They are organized into "web_scraping" and "subsidy_cleaning" folders. The web_scraping folder contains two .R scraping files: "subsidy_scrape_basic_data.R" and "subsidy_scrape_detailed_data.R". The basic data .R file is for gathering subsidy-level from Subsidy Tracker; the detailed data .R file is for gathering firm-level. They make heavy use of the RSelenium and haven packages in R. Since I am not a programmer, they are probably riddled with inefficiencies at either the code level or the concept/architecture level. But, they get the job done.< br/>

The folders in this repo also contain the datasets I scraped and cleaned. In the web_scraping folder, you can find the large .RData objects I used to store my scraping results. There are four of them, each reflecting scraping results from a single iteration of the web scraper (which took quite a long time to finish). Most people will probably not find value in these objects. In the subsidy_cleaning folder, you can find the polished and unpolished datasets I made from the .RData objects.






